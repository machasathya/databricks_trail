{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4526be3f-a0ae-46a0-95f0-b358dac48ddf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Access the via abfss directly without the mount\n",
    "# 1. Configure Spark to access ADLS Gen2 via OAuth\n",
    "spark.conf.set(\"fs.azure.account.auth.type.samagnadev.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(\"fs.azure.account.oauth.provider.type.samagnadev.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.id.samagnadev.dfs.core.windows.net\", \"354e564a-6750-407d-bd2b-31aec79a0a4f\")\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.secret.samagnadev.dfs.core.windows.net\", \"DrS8Q~XtsAnlppppcT14RxUTCMP~w-HE2nCEQc5-\")\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.endpoint.samagnadev.dfs.core.windows.net\", \"https://login.microsoftonline.com/7af2a929-b20f-4b9e-afe3-e02c2a070412/oauth2/token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2de80363-573a-4878-9b8c-90829eb59f72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "silver_path = \"abfss://practice@samagnadev.dfs.core.windows.net/silver/sales_data\"\n",
    "df_silver = spark.read.format(\"delta\").load(silver_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7942461-2611-43cc-805e-0a3a143e2867",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "jdbc_url = \"jdbc:sqlserver://sqlserver1q.database.windows.net:1433;database=sqldatabase;\"\n",
    "jdbc_user = \"sathya\"\n",
    "jdbc_pwd = \"Macha@123\"\n",
    "jdbc_props = {\"user\": jdbc_user, \"password\": jdbc_pwd, \"driver\":\"com.microsoft.sqlserver.jdbc.SQLServerDriver\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d38efcaf-6dbe-4c4d-b085-caf29efba7b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, date_format, max as spark_max\n",
    "\n",
    "# -------------------------------------------\n",
    "# 6) Build dimensions (SCD-1) and date dim\n",
    "# -------------------------------------------\n",
    "\n",
    "dim_customer = df_silver.select(\n",
    "    \"CustomerID\",\n",
    "    \"CompanyName\",\n",
    "    \"Industry\",\n",
    "    \"ContactName\",\n",
    "    \"CustomerCity\",\n",
    "    \"CustomerStateProvince\",\n",
    "    \"CustomerCountryRegion\",\n",
    "    \"CustomerType\"\n",
    ").dropDuplicates([\"CustomerID\"])\n",
    "\n",
    "dim_product = df_silver.select(\n",
    "    \"ProductID\",\n",
    "    \"ProductName\",\n",
    "    \"ProductCategory\",\n",
    "    \"ProductSubCategory\",\n",
    "    \"Weight\",\n",
    "    \"WeightUnit\",\n",
    "    \"DiscontinuedFlag\"\n",
    ").dropDuplicates([\"ProductID\"])\n",
    "\n",
    "dim_date = (\n",
    "    df_silver.select(col(\"OrderDate\").cast(\"date\").alias(\"DateValue\"))\n",
    "    .dropDuplicates()\n",
    "    .withColumn(\"DateKey\", date_format(col(\"DateValue\"), \"yyyyMMdd\").cast(\"int\"))\n",
    "    .withColumn(\"Year\", date_format(col(\"DateValue\"), \"yyyy\").cast(\"int\"))\n",
    "    .withColumn(\"Month\", date_format(col(\"DateValue\"), \"MM\").cast(\"int\"))\n",
    "    .withColumn(\"Day\", date_format(col(\"DateValue\"), \"dd\").cast(\"int\"))\n",
    ")\n",
    "\n",
    "# -------------------------------------------\n",
    "# 7) Write staging dims to SQL via JDBC\n",
    "# -------------------------------------------\n",
    "\n",
    "dim_customer.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .jdbc(url=jdbc_url, table=\"stg_DimCustomer\", properties=jdbc_props)\n",
    "\n",
    "dim_product.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .jdbc(url=jdbc_url, table=\"stg_DimProduct\", properties=jdbc_props)\n",
    "\n",
    "dim_date.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .jdbc(url=jdbc_url, table=\"stg_DimDate\", properties=jdbc_props)\n",
    "\n",
    "# -------------------------------------------\n",
    "# 8) Prepare fact staging (natural keys)\n",
    "# -------------------------------------------\n",
    "\n",
    "fact = (\n",
    "    df_silver\n",
    "    .withColumn(\"DateKey\", date_format(col(\"OrderDate\"), \"yyyyMMdd\").cast(\"int\"))\n",
    "    .withColumn(\"SalesAmount\", (col(\"QuantitySold\") * col(\"UnitPrice\")).cast(\"double\"))\n",
    "    .select(\n",
    "        \"DateKey\",\n",
    "        \"SalespersonKey\",\n",
    "        \"WarehouseKey\",\n",
    "        \"CustomerID\",\n",
    "        \"ProductID\",\n",
    "        \"SalesOrderID\",\n",
    "        \"SalesOrderDetailID_Source\",\n",
    "        \"QuantitySold\",\n",
    "        \"UnitPrice\",\n",
    "        \"StandardCost\",\n",
    "        \"SalesAmount\",\n",
    "        \"DiscountAmount\",\n",
    "        \"TaxAmount\"\n",
    "    )\n",
    ")\n",
    "\n",
    "fact.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .jdbc(url=jdbc_url, table=\"stg_FactSalesOrderDetail_Natural\", properties=jdbc_props)\n",
    "\n",
    "# -------------------------------------------\n",
    "# 9) Return max CreatedTimestamp for logging\n",
    "# -------------------------------------------\n",
    "\n",
    "max_ts = df_silver.agg(spark_max(\"CreatedTimestamp\").alias(\"max_ts\")).collect()[0][\"max_ts\"]\n",
    "\n",
    "print(\"Max processed CreatedTimestamp:\", max_ts)\n",
    "\n",
    "dbutils.notebook.exit(str(max_ts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fa964b62-af65-43eb-9bb4-5bcba2fb943c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "gold_agg",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
